import argparse
import os
import torch
from networks.recursive_cascade_networks import RecursiveCascadeNetwork
from torch.optim import Adam
from torch.optim.lr_scheduler import StepLR
from metrics.losses import total_loss
from data_util.ctscan import sample_generator
import matplotlib.pyplot as plt
from matplotlib.collections import LineCollection
import numpy as np

parser = argparse.ArgumentParser()
parser.add_argument('--b', "--batch_size", type=int, default=1)
parser.add_argument('--n', "--n_cascades", type=int, default=5)
parser.add_argument('--e', "--n_epochs", type=int, default=100)
parser.add_argument('--i', "--n_iters_train", type=int, default=2000)
parser.add_argument('--iv', "--n_iters_val", type=int, default=2000)
parser.add_argument('--c', "--checkpoint_frequency", type=int, default=20)
parser.add_argument('--fixed_sample', type=int, default=100)
args = parser.parse_args()


def identify_axes(ax_dict, fontsize=48):
    kw = dict(ha="center", va="center", fontsize=fontsize, color="darkgrey")
    for k, ax in ax_dict.items():
        ax.text(0.5, 0.5, k, transform=ax.transAxes, **kw)


def plot_grid(ax, flow, factor=10):
    """ Plot the grid generated by a flow. The displacement can be too small, so we add a scale factor"""
    grid = factor * flow[:, ::8, ::8]
    lin_range = np.linspace(0, 512, 64)
    x, y = np.meshgrid(lin_range, lin_range)
    x = x + grid[0, ...]
    y = y + grid[1, ...]
    y = y

    segs1 = np.stack((x, y), axis=2)
    segs2 = segs1.transpose(1, 0, 2)
    ax.add_collection(LineCollection(segs1, color='black', linewidths=0.8))
    ax.add_collection(LineCollection(segs2, color='black', linewidths=0.8))
    ax.autoscale()


def generate_plots(fixed, moving, warped, flows, train_loss, val_loss, reg_loss, epoch):
    """ Save some images and plots during training"""
    moving = moving.detach().cpu().numpy()
    fixed = fixed.detach().cpu().numpy()
    warped = [w.detach().cpu().numpy() for w in warped]
    flows = [f.detach().cpu().numpy() for f in flows]

    fig = plt.figure(constrained_layout=True, figsize=(4 * 5, 4 * 3))
    ax_dict = fig.subplot_mosaic("""
                                 FABCD
                                 LGHIE
                                 MKJWX
                                 """)

    ax_dict['F'].imshow(moving[0, 0, ...], cmap='gray')
    ax_dict['F'].set_title('Moving')

    ax_dict['W'].imshow(fixed[0, 0, ...], cmap='gray')
    ax_dict['W'].set_title('Fixed')

    for i, ax_name in enumerate(list("ABCDEX")):
        ax_dict[ax_name].imshow(warped[i][0, 0, ...], cmap='gray')
        if ax_name == "A":
            ax_dict[ax_name].set_title("Affine")
        else:
            ax_dict[ax_name].set_title(f"Cascade {i}")

    ax_dict['L'].plot(train_loss, color='red', label='train_loss')
    ax_dict['L'].plot(val_loss, label='val_loss', color='blue')
    ax_dict['L'].plot(reg_loss, label='train_reg_loss', color='green')
    ax_dict['L'].set_title("Losses")
    ax_dict['L'].grid()
    ax_dict['L'].set_xlim(0, args.e)
    ax_dict['L'].legend(loc='upper right')
    ax_dict['L'].scatter(len(train_loss) - 1, train_loss[-1], s=20, color='red')
    ax_dict['L'].scatter(len(val_loss) - 1, val_loss[-1], s=20, color='blue')
    ax_dict['L'].scatter(len(reg_loss) - 1, reg_loss[-1], s=20, color='green')

    for i, ax_name in enumerate(list("GHIJKM")):
        plot_grid(ax_dict[ax_name], flows[i][0, ...])
        if ax_name == "G":
            ax_dict[ax_name].set_title("Affine")
        else:
            ax_dict[ax_name].set_title(f"Cascade {i}")

    plt.suptitle(f"Epoch {epoch}")
    plt.savefig(f'./ckp/visualization/epoch_{epoch}.png')


def main():
    if not os.path.exists('./ckp/model_wts'):
        print("Creating ckp dir")
        os.makedirs('./ckp/model_wts')

    if not os.path.exists('./ckp/visualization'):
        print("Creating visualization dir")
        os.makedirs('./ckp/visualization')

    model = RecursiveCascadeNetwork(n_cascades=args.n, im_size=(512, 512))
    trainable_params = []
    for submodel in model.stems:
        trainable_params += list(submodel.parameters())

    trainable_params += list(model.reconstruction.parameters())

    optim = Adam(trainable_params, lr=1e-4)
    scheduler = StepLR(optimizer=optim, step_size=10, gamma=0.96)
    train_generator = iter(sample_generator('./train.txt', batch_size=args.b))
    val_generator = iter(sample_generator('./validation.txt', batch_size=args.b))

    # Saving the losses
    train_loss_log = []
    reg_loss_log = []
    val_loss_log = []

    for epoch in range(1, args.e + 1):
        print(f"-----Epoch {epoch} / {args.e}-----")
        train_epoch_loss = 0
        train_reg_loss = 0
        vis_batch = []
        model.train()
        for iteration in range(1, args.i):
            if iteration % int(0.1 * args.i) == 0:
                print(f"\t-----Iteration {iteration} / {args.i} -----")
            optim.zero_grad()
            fixed, moving = next(train_generator)
            fixed = fixed.cuda()
            moving = moving.cuda()
            warped, flows = model(fixed, moving)
            loss = total_loss(fixed, warped[-1], flows)
            loss.backward()
            optim.step()

            train_epoch_loss += loss.item()
            train_reg_loss += reg.item()

            if iteration == args.fixed_sample:
                vis_batch.append(fixed)
                vis_batch.append(moving)
                vis_batch.append(warped)
                vis_batch.append(flows)

        train_loss_log.append(train_epoch_loss / args.i)
        reg_loss_log.append(train_reg_loss / args.i)

        model.eval()
        print(f">>>>> Validation <<<<<")

        val_epoch_loss = 0
        for iteration in range(1, args.iv):
            if iteration % int(0.1 * args.iv) == 0:
                print(f"\t-----Iteration {iteration} / {args.iv} -----")

            with torch.no_grad():
                fixed, moving = next(val_generator)
                fixed = fixed.cuda()
                moving = moving.cuda()
                warped, flows = model(fixed, moving)
                sim, reg = total_loss(fixed, warped[-1], flows)
                loss = sim + reg
                val_epoch_loss += loss.item()

        val_loss_log.append(val_epoch_loss / args.iv)

        scheduler.step()

        if epoch % args.c == 0:
            ckp = {}
            for i, submodel in enumerate(model.stems):
                ckp[f"cascade {i}"] = submodel.state_dict()

            ckp['train_loss'] = train_loss_log
            ckp['val_loss'] = val_loss_log
            ckp['epoch'] = epoch

            torch.save(ckp, f'./ckp/model_wts/epoch_{epoch}.pth')

        generate_plots(vis_batch[0], vis_batch[1], vis_batch[2], vis_batch[3], train_loss_log, val_loss_log,
                       reg_loss_log, epoch)
